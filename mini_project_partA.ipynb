{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "colab": {
   "name": "mini_project_partA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MINI PROJECT â€” APPLIED DATA ANALYSIS & MODELING (PART A)\n",
    "\n",
    "Airline On-Time Performance & Passenger Experience\n",
    "\n",
    "This Colab notebook loads the Kaggle \"Airline Delay and Cancellation Data 2019\" (flights.csv), performs EDA, runs hypothesis tests between airlines, fits simple and multiple regression models, and saves cleaned data + plots into a `results/` folder.\n",
    "\n",
    "Instructions:\n",
    "- If you want automatic download via Kaggle, create `~/.kaggle/kaggle.json` with your credentials (see next cell).\n",
    "- Run cells sequentially. Plots save to `results/plots/` and cleaned CSV to `results/cleaned_data/clean_flights.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: configure Kaggle credentials (only required for automatic download)\n",
    "Run these in Colab if you want the notebook to download the dataset automatically. Replace with your credentials."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# !mkdir -p ~/.kaggle\n",
    "# !echo '{\"username\":\"YOUR_KAGGLE_USERNAME\",\"key\":\"YOUR_KAGGLE_API_KEY\"}' > ~/.kaggle/kaggle.json\n",
    "# !chmod 600 ~/.kaggle/kaggle.json\n",
    "# !kaggle datasets download -d usdot/flight-delays\n",
    "# !unzip -o flight-delays.zip\n",
    "\n",
    "print('If you want to use Kaggle automatic download, uncomment and run the commands above and add your credentials.')"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "RESULTS_DIR = 'results'\n",
    "CLEANED_DIR = os.path.join(RESULTS_DIR, 'cleaned_data')\n",
    "PLOTS_DIR = os.path.join(RESULTS_DIR, 'plots')\n",
    "SUMMARY_DIR = os.path.join(RESULTS_DIR, 'summary_reports')\n",
    "for d in (RESULTS_DIR, CLEANED_DIR, PLOTS_DIR, SUMMARY_DIR):\n",
    "    os.makedirs(d, exist_ok=True)\n"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Load dataset (flights.csv)\n",
    "If you uploaded `flights.csv` to the Colab session, it will load. Otherwise enable Kaggle download earlier."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "CSV_FILE = 'flights.csv'\n",
    "if not os.path.exists(CSV_FILE):\n",
    "    raise FileNotFoundError(\"Please upload 'flights.csv' to the Colab session or enable Kaggle download.\")\n",
    "\n",
    "df = pd.read_csv(CSV_FILE, low_memory=False)\n",
    "print('Loaded', CSV_FILE, 'shape =', df.shape)\n"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Initial inspection"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "display(df.head())\n",
    "print('\\nInfo:')\n",
    "print(df.info())\n",
    "print('\\nMissing values (top 20):')\n",
    "print(df.isnull().sum().sort_values(ascending=False).head(20))\n"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Select relevant columns and clean\n",
    "We map common column names to canonical names and build a working DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "col_map_candidates = {\n",
    "    'Year': ['Year', 'year'],\n",
    "    'Month': ['Month', 'month'],\n",
    "    'Day': ['DayofMonth', 'Day', 'day'],\n",
    "    'Carrier': ['UniqueCarrier', 'Carrier', 'OP_UNIQUE_CARRIER', 'Reporting_Airline'],\n",
    "    'FlightNum': ['FlightNum', 'FlightNumber', 'Flight'],\n",
    "    'Origin': ['Origin', 'ORIGIN', 'origin'],\n",
    "    'Dest': ['Dest', 'DEST', 'dest'],\n",
    "    'DepDelay': ['DepDelay', 'DEP_DELAY', 'DepDelayMinutes'],\n",
    "    'ArrDelay': ['ArrDelay', 'ARR_DELAY', 'ArrDelayMinutes'],\n",
    "    'Distance': ['Distance', 'DISTANCE'],\n",
    "    'Cancelled': ['Cancelled', 'CANCELLED'],\n",
    "}\n",
    "found_cols = {}\n",
    "for canonical, candidates in col_map_candidates.items():\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            found_cols[canonical] = c\n",
    "            break\n",
    "\n",
    "print('Found columns mapping:', found_cols)\n",
    "use_cols = list(found_cols.values())\n",
    "working = df[use_cols].copy()\n",
    "inv_map = {v: k for k, v in found_cols.items()}\n",
    "working.rename(columns=inv_map, inplace=True)\n",
    "\n",
    "for col in ('ArrDelay', 'DepDelay', 'Distance', 'Month', 'Day', 'Year'):\n",
    "    if col in working.columns:\n",
    "        working[col] = pd.to_numeric(working[col], errors='coerce')\n",
    "\n",
    "if 'Cancelled' in working.columns:\n",
    "    working['Cancelled'] = pd.to_numeric(working['Cancelled'], errors='coerce').fillna(0).astype(int)\n",
    "else:\n",
    "    working['Cancelled'] = 0\n",
    "\n",
    "working['Delay'] = working.get('ArrDelay').fillna(working.get('DepDelay'))\n",
    "working = working[working['Delay'].notna()]\n",
    "print('Rows after keeping those with delay info:', working.shape[0])\n"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Prepare dataset for analysis\n",
    "- Focus on non-cancelled flights\n",
    "- Remove extreme outliers (> 6 hours)\n",
    "- Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "delay_df = working[working['Cancelled'] == 0].copy()\n",
    "delay_df = delay_df[delay_df['Delay'].abs() < 360].copy()\n",
    "before = delay_df.shape[0]\n",
    "delay_df.drop_duplicates(inplace=True)\n",
    "print('Dropped duplicates:', before - delay_df.shape[0])\n",
    "print('Final rows for analysis:', delay_df.shape)\n"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Descriptive statistics and carrier summary"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "display(delay_df['Delay'].describe(percentiles=[0.01,0.05,0.25,0.5,0.75,0.95,0.99]))\n",
    "if 'Carrier' in delay_df.columns:\n",
    "    carrier_stats = (\n",
    "        delay_df.groupby('Carrier')['Delay']\n",
    "        .agg(['count','mean','median','std','min','max'])\n",
    "        .sort_values('count', ascending=False)\n",
    "    )\n",
    "    display(carrier_stats.head(10))\n"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Visualizations (histogram, boxplot, heatmap)\n",
    "Plots are saved under `results/plots/`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def save_fig(fig, fname):\n",
    "    path = os.path.join(PLOTS_DIR, fname)\n",
    "    fig.savefig(path, dpi=150, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    return path\n",
    "\n",
    "# Delay histogram\n",
    "fig = plt.figure()\n",
    "sns.histplot(delay_df['Delay'], bins=100, kde=True)\n",
    "plt.title('Delay distribution (minutes)')\n",
    "plt.xlabel('Delay (minutes)')\n",
    "plt.xlim(-60, 300)\n",
    "hist_path = save_fig(fig, 'delay_distribution.png')\n",
    "print('Saved:', hist_path)\n",
    "\n",
    "# Boxplot for top carriers\n",
    "if 'Carrier' in delay_df.columns:\n",
    "    top_carriers = delay_df['Carrier'].value_counts().nlargest(8).index.tolist()\n",
    "    subset = delay_df[delay_df['Carrier'].isin(top_carriers)]\n",
    "    fig = plt.figure(figsize=(12,6))\n",
    "    sns.boxplot(x='Carrier', y='Delay', data=subset, order=top_carriers)\n",
    "    plt.ylim(-60, 300)\n",
    "    box_path = save_fig(fig, 'airline_comparison_boxplot.png')\n",
    "    print('Saved:', box_path)\n",
    "\n",
    "# Correlation heatmap\n",
    "numeric_cols = [c for c in ('Delay','Distance','Month','Day') if c in delay_df.columns]\n",
    "fig = plt.figure()\n",
    "sns.heatmap(delay_df[numeric_cols].corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "heatmap_path = save_fig(fig, 'correlation_heatmap.png')\n",
    "print('Saved:', heatmap_path)\n"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Hypothesis testing: two-sample t-test and one-tailed\n",
    "Compare mean delays between the top two carriers (by count)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "tt_test_summary = {}\n",
    "if 'Carrier' in delay_df.columns and delay_df['Carrier'].nunique() >= 2:\n",
    "    top_two = delay_df['Carrier'].value_counts().nlargest(2).index.tolist()\n",
    "    c1, c2 = top_two[0], top_two[1]\n",
    "    d1 = delay_df[delay_df['Carrier'] == c1]['Delay'].dropna()\n",
    "    d2 = delay_df[delay_df['Carrier'] == c2]['Delay'].dropna()\n",
    "    # sample for speed\n",
    "    max_n = 5000\n",
    "    if len(d1) > max_n:\n",
    "        d1 = d1.sample(max_n, random_state=1)\n",
    "    if len(d2) > max_n:\n",
    "        d2 = d2.sample(max_n, random_state=1)\n",
    "    t_stat, p_two = stats.ttest_ind(d1, d2, equal_var=False, nan_policy='omit')\n",
    "    p_one = p_two/2 if t_stat < 0 else 1 - p_two/2\n",
    "    tt_test_summary = {'carrier_a': c1, 'carrier_b': c2, 't_stat': float(t_stat), 'p_two': float(p_two), 'p_one': float(p_one)}\n",
    "    print('T-test summary:', tt_test_summary)\n",
    "else:\n",
    "    print('Not enough carriers to run t-test')\n"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Regression models\n",
    "- Simple: Delay ~ Distance\n",
    "- Multiple: Delay ~ Distance + Month + Carrier (one-hot top carriers)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "regression_info = {}\n",
    "# Simple regression\n",
    "if 'Distance' in delay_df.columns:\n",
    "    X = delay_df[['Distance']].values.reshape(-1,1)\n",
    "    y = delay_df['Delay'].values\n",
    "    idx = np.random.RandomState(0).choice(np.arange(len(X)), size=min(20000, len(X)), replace=False)\n",
    "    Xs, ys = X[idx], y[idx]\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(Xs, ys, test_size=0.2, random_state=42)\n",
    "    lr = LinearRegression().fit(X_tr, y_tr)\n",
    "    y_pred = lr.predict(X_te)\n",
    "    regression_info['simple'] = {'coef': float(lr.coef_[0]), 'intercept': float(lr.intercept_), 'r2': float(r2_score(y_te, y_pred))}\n",
    "    fig = plt.figure()\n",
    "    plt.scatter(X_te[:,0], y_te, alpha=0.2, s=10)\n",
    "    x_line = np.linspace(X_te.min(), X_te.max(), 100)\n",
    "    plt.plot(x_line, lr.intercept_ + lr.coef_[0]*x_line, color='red')\n",
    "    save_fig(fig, 'regression_fit_distance.png')\n",
    "\n",
    "# Multiple regression\n",
    "if 'Carrier' in delay_df.columns:\n",
    "    sample = delay_df.sample(n=min(20000, len(delay_df)), random_state=2)\n",
    "    X_multi = pd.DataFrame()\n",
    "    if 'Distance' in sample.columns:\n",
    "        X_multi['Distance'] = sample['Distance']\n",
    "    if 'Month' in sample.columns:\n",
    "        X_multi['Month'] = sample['Month']\n",
    "    top_n = 8\n",
    "    top_carriers = sample['Carrier'].value_counts().nlargest(top_n).index.tolist()\n",
    "    sample['Carrier_top'] = sample['Carrier'].where(sample['Carrier'].isin(top_carriers), 'OTHER')\n",
    "    dummies = pd.get_dummies(sample['Carrier_top'], prefix='Carrier', drop_first=True)\n",
    "    X_multi = pd.concat([X_multi, dummies], axis=1)\n",
    "    y_multi = sample['Delay'].values\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X_multi, y_multi, test_size=0.2, random_state=42)\n",
    "    lr_multi = LinearRegression().fit(X_tr, y_tr)\n",
    "    y_pred_m = lr_multi.predict(X_te)\n",
    "    regression_info['multiple'] = {'r2': float(r2_score(y_te, y_pred_m)), 'mse': float(mean_squared_error(y_te, y_pred_m))}\n",
    "    print('Regression results:', regression_info)\n"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10) Save cleaned dataset, plots, and summary"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "delay_df.to_csv(os.path.join(CLEANED_DIR, 'clean_flights.csv'), index=False)\n",
    "\n",
    "summary_lines = [\n",
    "    'Airline On-Time Performance - Part A',\n",
    "    f'Generated: {datetime.utcnow().isoformat()} UTC',\n",
    "    f'Original shape: {df.shape}',\n",
    "    f'Rows after cleaning: {delay_df.shape}',\n",
    "    '',\n",
    "    'Delay descriptive statistics:',\n",
    "    desc_delay.to_string(),\n",
    "    '',\n",
    "    'T-test summary:',\n",
    "    str(tt_test_summary),\n",
    "    '',\n",
    "    'Regression summary:',\n",
    "    str(regression_info)\n",
    "]\n",
    "with open(os.path.join(SUMMARY_DIR, 'airline_summary.txt'), 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(summary_lines))\n",
    "\n",
    "print('Saved cleaned CSV and summary. Check the results/ folder for plots and outputs.')\n"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of notebook\n",
    "\n",
    "If you want this notebook saved directly into your GitHub repo, download it (File > Download .ipynb) and add it to your repo, or I can provide the commit commands to run locally."
   ]
  }
 ],
 "cells_hidden": false
}
